{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras, time, sys, os, gc\n",
    "\n",
    "DTYPE_OP = 'float32'\n",
    "keras.backend.set_floatx(DTYPE_OP)\n",
    "\n",
    "if DTYPE_OP == 'float64':\n",
    "    keras.backend.set_epsilon(np.finfo(np.float64).eps)\n",
    "elif DTYPE_OP == 'float32':\n",
    "    keras.backend.set_epsilon(np.finfo(np.float32).eps)\n",
    "    \n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL Variables\n",
    "BATCH_SIZE = 128 #rodrigues = BATCH_SIZE = 64\n",
    "EPOCHS_BASE = 50\n",
    "OPT = 'adam' #optimizer for neural network \n",
    "TOL = 3e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./data/Sentiment/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ... visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ... visualize data with annotations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphics and distribution? what is T_data\n",
    "N_ann = np.sum(y_obs != -1,axis=1) #distribucion de anotaciones con este valor\n",
    "\n",
    "sns.countplot(N_ann)\n",
    "plt.show()\n",
    "print(\"Minimum annotations in a example: \",N_ann.min())\n",
    "print(\"Maximum annotations in a example: \",N_ann.max())\n",
    "print(\"T data (expected value): \",N_ann.mean())\n",
    "\n",
    "print(\"Graphics of T weights\")\n",
    "plt.hist(T_weights)\n",
    "plt.show()\n",
    "\n",
    "plt.boxplot(T_weights)\n",
    "plt.show()\n",
    "print(\"Minimum annotations by an annotator: \",T_weights.min())\n",
    "print(\"Maximum annotations by an annotator: \",T_weights.max())\n",
    "print(\"Mean annotations by an annotator: \",T_weights.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Matrix representation of both scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeE.representation import set_representation\n",
    "\n",
    "y_obs_categorical = set_representation(y_obs,'onehot') \n",
    "print(\"Individual representation shape (N,T,K)= \",y_obs_categorical.shape)\n",
    "\n",
    "r_obs = set_representation(y_obs,\"global\")\n",
    "print(\"Global representation shape (N,K)= \",r_obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Real Confusion matrices\n",
    "Generate the individual and global confusion matrices based on the sampled annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeE.utils import generate_Individual_conf\n",
    "\n",
    "conf_matrix_I = generate_Individual_conf(Z_train, y_obs_categorical)\n",
    "print(\"Individual matrix shape (T,K,K)= \",conf_matrix_I.shape)\n",
    "conf_matrix_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeE.utils import generate_Global_conf\n",
    "\n",
    "#confe_matrix_G = generate_Global_conf(Z_train, y_obs_categorical)\n",
    "confe_matrix_G = generate_Global_conf(Z_train, r_obs)\n",
    "print(\"Global matrix shape (K,K)= \",confe_matrix_G.shape)\n",
    "\n",
    "from codeE.utils import plot_confusion_matrix\n",
    "plot_confusion_matrix(confe_matrix_G, [\"negative\", \"positive\"], title= \"Global Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delta Convergence criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from codeE.learning_models import LogisticRegression_Sklearn,LogisticRegression_Keras,MLP_Keras\n",
    "#deep learning\n",
    "from codeE.learning_models import default_CNN,default_RNN,CNN_simple, RNN_simple, Clonable_Model\n",
    "\n",
    "from codeE.utils import EarlyStopRelative\n",
    "ourCallback = EarlyStopRelative(monitor='loss',patience=1,min_delta=TOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper Bound Model -- ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_train_onehot = keras.utils.to_categorical(Z_train)\n",
    "\n",
    "model_UB = ....\n",
    "\n",
    "model_UB.compile(loss='categorical_crossentropy',optimizer=OPT)\n",
    "hist = model_UB.fit(X_train,Z_train_onehot,epochs=EPOCHS_BASE,batch_size=BATCH_SIZE,verbose=0,callbacks=[ourCallback])\n",
    "print(\"Trained Ideal Model, Epochs to converge =\",len(hist.epoch))\n",
    "clone_UB = Clonable_Model(model_UB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeE.evaluation import accuracy_model\n",
    "print(\"Train accuracy of model =\", accuracy_model(model_UB, Xstd_train, Z_train))\n",
    "print(\"Test accuracy of model =\", accuracy_model(model_UB, Xstd_test, Z_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation methods (without predictive model)\n",
    "* Majority Voting (MV)\n",
    "* Weighted Majority Voting\n",
    "* Dawid and Skene (DS): with inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from codeE.methods import LabelAggregation\n",
    "label_A = LabelAggregation(scenario=\"global\")\n",
    "\n",
    "mv_soft = label_A.infer(  r_obs, 'softMV')\n",
    "mv_hard = label_A.predict(r_obs, 'hardMV')\n",
    "mv_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from codeE.methods import LabelAggregation\n",
    "label_A = LabelAggregation(scenario=\"individual\")\n",
    "\n",
    "mv_soft = label_A.infer(  y_obs_categorical, 'softMV')\n",
    "mv_hard = label_A.predict(y_obs_categorical, 'hardMV')\n",
    "mv_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ACC MV on train:\",np.mean(mv_hard==Z_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate confusion matrix to understand MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeE.utils import generate_confusionM\n",
    "prob_Yz = generate_confusionM(mv_soft, r_obs) #confusion matrix of all annotators\n",
    "\n",
    "from codeE.utils import plot_confusion_matrix\n",
    "plot_confusion_matrix(prob_Yz, np.arange(8), title= \"soft-MV confusion matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_Yz = generate_confusionM(mv_hard, r_obs) #confusion matrix of all annotators\n",
    "plot_confusion_matrix(prob_Yz, np.arange(8), title= \"hard-MV confusion matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics to compare the estimation of confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeE.utils import compare_conf_mats\n",
    "compare_conf_mats(prob_Yz, true_conf_mat=confe_matrix_G, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeE.evaluation import D_JS, D_KL, D_NormF\n",
    "print(\"Global D (based on Jensen Shannon) =\",D_JS(confe_matrix_G, prob_Yz))\n",
    "print(\"Global D (based on normalized Frobenius) =\",D_NormF(confe_matrix_G, prob_Yz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeE.methods import LabelAggregation\n",
    "label_A = LabelAggregation(scenario=\"individual\")\n",
    "\n",
    "Wmv_soft = label_A.infer(y_obs_categorical, 'softMV', weights=T_weights)\n",
    "Wmv_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ACC MV on train:\",np.mean(Wmv_soft.argmax(axis=-1)==Z_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dawid and Skene model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from codeE.methods import LabelInference_EM as DS\n",
    "\n",
    "DS_model = DS(init_Z='softmv') \n",
    "#DS_model = DS(init_Z='softmv', priors=\"None\", fast=False) \n",
    "    \n",
    "# if you want you can initialize step E forcing some method\n",
    "#DS_model.init_E(y_obs_categorical)\n",
    "#DS_model.get_qestimation()\n",
    "\n",
    "hist = DS_model.fit(y_obs_categorical, max_iter=EPOCHS_BASE, tolerance=TOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"p(z) = \",DS_model.get_marginalZ())\n",
    "\n",
    "ds_labels = DS_model.infer() #could infer/predict on the trainint set only\n",
    "ds_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ACC D&S on train:\",np.mean(ds_labels.argmax(axis=1)==Z_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_Yzt = DS_model.get_confusionM() # estimate of conf_matrix_I\n",
    "\n",
    "from codeE.evaluation import Individual_D, D_JS, D_NormF\n",
    "print(\"Individual D_JS = \",Individual_D(conf_matrix_I, prob_Yzt, D=D_JS))\n",
    "print(\"Individual D_NormF = \",Individual_D(conf_matrix_I, prob_Yzt, D=D_NormF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Phase\n",
    "train a predictive model over the inference/aggregation label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeE.evaluation import accuracy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mvsoft = clone_UB.get_model()\n",
    "model_mvsoft.compile(loss='categorical_crossentropy',optimizer=OPT)\n",
    "hist=model_mvsoft.fit(Xstd_train, mv_soft, epochs=EPOCHS_BASE,batch_size=BATCH_SIZE,verbose=0,callbacks=[ourCallback])\n",
    "print(\"Trained model over soft-MV, Epochs to converge =\",len(hist.epoch))\n",
    "\n",
    "#Z_train_pred = model_mvsoft.predict_classes(Xstd_train)\n",
    "#Z_test_pred = model_mvsoft.predict_classes(Xstd_test)\n",
    "\n",
    "print(\"Train accuracy of model =\", accuracy_model(model_mvsoft, Xstd_train, Z_train))\n",
    "print(\"Test accuracy of model =\", accuracy_model(model_mvsoft, Xstd_test, Z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_hard_onehot = keras.utils.to_categorical(mv_hard, num_classes=Kl)\n",
    "mv_hard_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mvhard = clone_UB.get_model()\n",
    "model_mvhard.compile(loss='categorical_crossentropy',optimizer=OPT)\n",
    "hist=model_mvhard.fit(Xstd_train, mv_hard_onehot, epochs=EPOCHS_BASE,batch_size=BATCH_SIZE,verbose=0,callbacks=[ourCallback])\n",
    "print(\"Trained model over hard-MV, Epochs to converge =\",len(hist.epoch))\n",
    "\n",
    "#Z_train_pred = model_mvhard.predict_classes(Xstd_train)\n",
    "#Z_test_pred = model_mvhard.predict_classes(Xstd_test)\n",
    "print(\"Train accuracy of model =\", accuracy_model(model_mvhard, Xstd_train, Z_train))\n",
    "print(\"Test accuracy of model =\", accuracy_model(model_mvhard, Xstd_test, Z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ds = clone_UB.get_model()\n",
    "model_ds.compile(loss='categorical_crossentropy',optimizer=OPT)\n",
    "hist=model_ds.fit(Xstd_train, ds_labels, epochs=EPOCHS_BASE,batch_size=BATCH_SIZE,verbose=0,callbacks=[ourCallback])\n",
    "print(\"Trained model over D&S, Epochs to converge =\",len(hist.epoch))\n",
    "\n",
    "#Z_train_pred = model_ds.predict_classes(Xstd_train)\n",
    "#Z_test_pred = model_ds.predict_classes(Xstd_test)\n",
    "\n",
    "print(\"Train accuracy of model =\", accuracy_model(model_ds, Xstd_train, Z_train))\n",
    "print(\"Test accuracy of model =\", accuracy_model(model_ds, Xstd_test, Z_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raykar Model (joint predict model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_R = clone_UB.get_model()\n",
    "model_R.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from codeE.methods import ModelInference_EM as Raykar\n",
    "R_model = Raykar(init_Z=\"softmv\")\n",
    "#R_model = Raykar(init_Z=\"softmv\", priors='laplace', n_init_Z=3)\n",
    "\n",
    "args = {'epochs':1, 'batch_size':BATCH_SIZE, 'optimizer':OPT}\n",
    "R_model.set_model(model_R, **args)\n",
    "\n",
    "# if you want you can initialize step E forcing some method\n",
    "#R_model.init_E(y_obs_categorical, method=\"hardmv\")\n",
    "#R_model.get_qestimation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#logL_hists,i_r = R_model.multiples_run(20, Xstd_train,y_obs_categorical,\n",
    "#                                        max_iter=EPOCHS_BASE, tolerance=TOL)\n",
    "\n",
    "R_model.fit(Xstd_train, y_obs_categorical, runs=20, max_iter=EPOCHS_BASE, tolerance=TOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "for a, value in enumerate(logL_hists): \n",
    "    if a != i_r:\n",
    "        plt.plot(range(len(value)),value,'.-')\n",
    "plt.plot(range(len(logL_hists[i_r])),logL_hists[i_r],'o-',markersize=10,label=\"Selected run\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raykar_fx = R_model.get_basemodel()\n",
    "\n",
    "from codeE.evaluation import accuracy_model\n",
    "print(\"Train accuracy of model =\", accuracy_model(raykar_fx, Xstd_train, Z_train))\n",
    "print(\"Test accuracy of model =\", accuracy_model(raykar_fx, Xstd_test, Z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_Yzt = R_model.get_confusionM() # estimate of conf_matrix_I\n",
    "\n",
    "from codeE.evaluation import Individual_D, D_JS, D_NormF\n",
    "print(\"Individual D_JS = \",Individual_D(conf_matrix_I, prob_Yzt, D=D_JS))\n",
    "print(\"Individual D_NormF = \",Individual_D(conf_matrix_I, prob_Yzt, D=D_NormF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_Yz = prob_Yzt.mean(axis=0)\n",
    "from codeE.evaluation import D_JS, D_KL, D_NormF\n",
    "print(\"Global D (based on Jensen Shannon) =\",D_JS(confe_matrix_G, prob_Yz))\n",
    "print(\"Global D (based on normalized Frobenius) =\",D_NormF(confe_matrix_G, prob_Yz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#how good estimate the annotations ?\n",
    "prob_Y_xt = R_model.get_predictions_annot(Xstd_train)\n",
    "- np.sum(np.sum(y_obs_categorical * np.log(prob_Y_xt +1e-7 ), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Proposed - CMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_seted = 3\n",
    "\n",
    "aux_model = clone_UB.get_model()\n",
    "aux_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from codeE.methods import CMM\n",
    "CMM_model = CMM(M=M_seted) \n",
    "#CMM_model = CMM(M=M_seted, init_Z='softmv', n_init_Z=3, priors=0) \n",
    "\n",
    "args = {'epochs':1, 'batch_size':BATCH_SIZE, 'optimizer':OPT}\n",
    "CMM_model.set_model(aux_model, **args)\n",
    "\n",
    "# if you want you can initialize step E forcing some method\n",
    "#CMM_model.init_E(r_obs, method=\"hardmv\")\n",
    "#CMM_model.get_qestimation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#logL_hists,i_r = CMM_model.multiples_run(20,Xstd_train,r_obs,\n",
    "#                                               max_iter=EPOCHS_BASE,tolerance=TOL)\n",
    "\n",
    "CMM_model.fit(Xstd_train, r_obs, runs =20,max_iter=EPOCHS_BASE,tolerance=TOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "for a, value in enumerate(logL_hists): #logL_hists\n",
    "    if a != i_r:\n",
    "        plt.plot(range(len(value)),value,'.-')\n",
    "plt.plot(range(len(logL_hists[i_r])),logL_hists[i_r],'o-',markersize=10,label=\"Selected run\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmm_fx = CMM_model.get_basemodel()\n",
    "\n",
    "from codeE.evaluation import accuracy_model\n",
    "print(\"Train accuracy of model =\", accuracy_model(cmm_fx, Xstd_train, Z_train))\n",
    "print(\"Test accuracy of model =\", accuracy_model(cmm_fx, Xstd_test, Z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_Yz = CMM_model.get_global_confusionM()\n",
    "\n",
    "from codeE.evaluation import D_JS, D_KL, D_NormF\n",
    "print(\"Global D (based on Jensen Shannon) =\",D_JS(confe_matrix_G, prob_Yz))\n",
    "print(\"Global D (based on normalized Frobenius) =\",D_NormF(confe_matrix_G, prob_Yz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"p(g) =\",CMM_model.get_alpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B = CMM_model.get_confusionM()\n",
    "for i in range(len(B)):\n",
    "    plot_confusion_matrix(B[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With priors= \"laplace\"\n",
    "smooth confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "for a, value in enumerate(logL_hists): #logL_hists\n",
    "    if a != i_r:\n",
    "        plt.plot(range(len(value)),value,'.-')\n",
    "plt.plot(range(len(logL_hists[i_r])),logL_hists[i_r],'o-',markersize=10,label=\"Selected run\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"p(g) =\",CMM_model.get_alpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B = CMM_model.get_confusionM()\n",
    "for i in range(len(B)):\n",
    "    plot_confusion_matrix(B[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## usar las otras funciones parao btener grupos de anotadoras.. en visualizacion!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_Yzt = []\n",
    "for t in range(y_obs.shape[1]):\n",
    "    conf = CMM_model.get_ann_confusionM(Xstd_train, y_obs[:,t])\n",
    "    \n",
    "    prob_Yzt.append(conf)\n",
    "prob_Yzt = np.asarray(prob_Yzt)\n",
    "prob_Yzt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Proposed - C-MoA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliar representation with variable number of annotations per annotator and identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_var, A_idx_var = set_representation(y_obs,\"onehotvar\")\n",
    "print(\"Individual sparse representation, variable shape (N,)= \",y_cat_var.shape)\n",
    "K = y_cat_var[0].shape[1]\n",
    "print(\"one-hot vectors of K-dimensions, K=\",K)\n",
    "y_cat_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_seted = 3\n",
    "\n",
    "aux_model = clone_UB.get_model()\n",
    "aux_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed annotators representation (non trainable embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T= np.concatenate(A_idx_var).max() +1\n",
    "A_rep = np.zeros((T, K))\n",
    "for i in range(N):\n",
    "    for l, t_idx in enumerate(A_idx_var[i]):\n",
    "        obs_t = y_cat_var[i][l].argmax(axis=-1)\n",
    "        A_rep[t_idx, obs_t] += 1\n",
    "print(\"shape of annotator representation (T, R_t)=\", A_rep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "group_model = Sequential()\n",
    "group_model.add(Embedding(T, K,input_length=1, \n",
    "                         trainable=False,weights=[A_rep]))\n",
    "#                         trainable=True))\n",
    "group_model.add(Reshape([K]))\n",
    "group_model.add(Dense(K*M_seted, activation='relu'))\n",
    "group_model.add(Dense(M_seted, activation='softmax'))\n",
    "group_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from codeE.methods import CMOA\n",
    "CMOA_model = CMOA(M=M_seted) \n",
    "#CMOA_model = CMOA(M=M_seted, init_Z='softmv', n_init_Z=0, n_init_G=0, priors=1) \n",
    "\n",
    "args = {'epochs':1, 'batch_size':BATCH_SIZE, 'optimizer':OPT}\n",
    "CMOA_model.set_model(aux_model, ann_model=group_model, **args)\n",
    "#CMOA_model.set_model(group_model) #you also can set it manually\n",
    "\n",
    "# if you want you can initialize step E forcing some method\n",
    "#CMOA_model.init_E(y_cat_var, A_idx_var, method=\"hardmv\")\n",
    "#CMOA_model.get_qestimation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#logL_hists,i_r = CMOA_model.multiples_run(20,Xstd_train,y_cat_var, A_idx_var, \n",
    "#                                   max_iter=EPOCHS_BASE,tolerance=TOL) \n",
    "\n",
    "CMOA_model.fit(Xstd_train, y_cat_var, A_idx_var, runs = 20,max_iter=EPOCHS_BASE,tolerance=TOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "for a, value in enumerate(logL_hists): #logL_hists\n",
    "    if a != i_r:\n",
    "        plt.plot(range(len(value)),value,'.-')\n",
    "plt.plot(range(len(logL_hists[i_r])),logL_hists[i_r],'o-',markersize=10,label=\"Selected run\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmoaK_fx = CMOA_model.get_basemodel()\n",
    "\n",
    "from codeE.evaluation import accuracy_model\n",
    "print(\"Train accuracy of model =\", accuracy_model(cmoaK_fx, Xstd_train, Z_train))\n",
    "print(\"Test accuracy of model =\", accuracy_model(cmoaK_fx, Xstd_test, Z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A = np.unique(np.concatenate(A_idx_var)).reshape(-1,1)\n",
    "prob_Yzt = CMOA_model.get_ann_confusionM(A) # estimate of conf_matrix_I\n",
    "\n",
    "from codeE.evaluation import Individual_D, D_JS, D_NormF\n",
    "print(\"Individual D_JS = \",Individual_D(conf_matrix_I, prob_Yzt, D=D_JS))\n",
    "print(\"Individual D_NormF = \",Individual_D(conf_matrix_I, prob_Yzt, D=D_NormF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B = CMOA_model.get_confusionM()\n",
    "for i in range(len(B)):\n",
    "    plot_confusion_matrix(B[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With priors= \"laplace\"\n",
    "smooth confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B = CMOA_model.get_confusionM()\n",
    "for i in range(len(B)):\n",
    "    plot_confusion_matrix(B[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mostrar el decaimeitno en performance al aumentar el numero de anotadoras"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:newpy3_tf1]",
   "language": "python",
   "name": "conda-env-newpy3_tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
