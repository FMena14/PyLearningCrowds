{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Notation\n",
    "\n",
    "\n",
    "### Supervised Scenario\n",
    "* Consider an input pattern $x \\in \\mathbb{X}$ observed with probability distribution $p(x)$ and a ground-truth label $z \\in \\mathbb{Z}$ observed with conditional probability distribution $p(z|x)$.\n",
    "* Given a finite sample $S=\\{\\left(x^{(1)},z^{(1)}\\right), \\ldots,\\left(x^{(N)},z^{(N)}\\right)\\}$, where $\\left(x^{(i)},z^{(i)}\\right) \\sim p(x,z)=p(z|x)p(x) \\, \\ \\forall i \\in [N]$. \n",
    "* Objective: estimate a predictive model $f(x)$ that maps $x \\rightarrow z$ or learn statistics of $p(z|x)$.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1204/0*qf-O7Jm1mmZrXYqA\" width=\"50%\" />\n",
    "    \n",
    "\n",
    "\n",
    "### Crowdsourcing scenario\n",
    "* Same objective that supervised scenario, but the ground-truth labels $z^{(i)}$ corresponding to the input patterns $x^{(i)}$ are not directly observed. \n",
    "* Consider labels $y \\in \\mathbb{Z}$ that do not follow the ground-truth distribution $p(z|x)$. Instead, they are generated from an unknown process $p(y^{(\\ell)}|x,z)$ that represents the annotator **ability** to detect the ground truth.\n",
    "\n",
    "> #### Individual\n",
    "* Consider multiple noise labels $\\mathcal{L}_i = \\{y_i^{(1)},\\ldots, y_i^{(T_i)}\\}$ given by $T_i$ annotators.\n",
    "* These annotations come from a subset $\\mathcal{A}_i$ of the set of all the annotators $\\mathcal{A}$ participating in the labelling process. ($T = |\\mathcal{A}|$ )\n",
    "* The annotator identity could be define as a variable: $a_{i}^{(\\ell)} \\in \\mathcal{A}$, con $\\mathcal{A} = \\{ 1, \\ldots, T\\}$, \n",
    "    * Then $p(y|x,z, a=\\ell) = p(y^{(\\ell)}|x,z)$\n",
    "* Given a sample $\\{(x_i, \\mathcal{L}_i )\\}_{i=1}^N$ or $\\{(x_i, (\\mathcal{L}_i, \\mathcal{A}_i) )\\}_{i=1}^N$\n",
    "\n",
    "> #### Global\n",
    "* Consider that we do not known or do not care which annotators provided the labels: we know $|\\mathcal{A}_i|$ but not $\\mathcal{A}_i$\n",
    "* Consider the number of times that all the annotators gives each possible labels: $r_{ij} \\in \\{0,1,\\ldots,T_i\\}$\n",
    "* Given a sample $\\{ (x_i,r_i) \\}_{i=1}^N$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Focus\n",
    "In this implementation, we study the pattern recognition case, that is, we let $\\mathbb{Z}$ be a small set of $K$ categories or classes $\\{c_1,c_2,\\ldots,c_K\\}$. \n",
    "\n",
    "---\n",
    "\n",
    "One also can define two scenarios based on the annotation density and assumptions:\n",
    "* **Dense**: \n",
    "    * All the annotators labels each data: $\\mathcal{A}_i = \\mathcal{A}$\n",
    "    * The implementation is simpler since fixed size matrices are assumed.\n",
    "* **Sparse**: \n",
    "    * The number of labels collected by data point and annotator varies: $|\\mathcal{A}_i| \\neq |\\mathcal{A}_j| < |\\mathcal{A}| = T$\n",
    "    * An appropiate implementation lead to computational efficiency.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
